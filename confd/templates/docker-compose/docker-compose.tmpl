# Generated by confd.
# Do not change this file, all changes will be lost. Change corresponding template.

version: '2.3'
services:

{{if not (exists "/no_grpc_stub")}}
  grpc-stub:
    container_name: grpc-stub
    image: kilda/grpc-stub:latest
    build:
      context: docker/grpc-stub/
      dockerfile: Dockerfile
    ports:
      - "50051:50051"
    command: python server.py
    networks:
      default:
        aliases:
          - grpc-stub.pendev
    mem_limit: ${GRPC_STUB_MEM_LIMIT:-67108864}
{{end}}

  lab-api:
    container_name: lab-api
    image: kilda_lab-service:latest
    build:
      context: docker/lab-service/
      dockerfile: Dockerfile
    ports:
    - "8288:8288"
    privileged: true
    command: api
    volumes:
    - /var/run/docker.sock:/var/run/docker.sock
    - /lib/modules:/lib/modules
    networks:
      default:
        aliases:
        - lab-api.pendev
    mem_limit: ${LAB_API_MEM_LIMIT:-256741824}

  odb1:
    container_name: odb1
    hostname: odb1.pendev
    image: orientdb:3.0.34
    command: dserver.sh
    volumes:
      - ./docker/orientdb/config/odb1:/orientdb/config
      - ./docker/orientdb/config/shared:/orientdb/config-shared
      - odb1_data:/orientdb/databases
    environment:
      ORIENTDB_ROOT_PASSWORD: root
      ORIENTDB_NODE_NAME: odb1.pendev
    ports:
      - 2424:2424
      - 2480:2480
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:2480/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      default:
        aliases:
          - odb1.pendev
{{if not (exists "/single_orientdb")}}

  odb2:
    container_name: odb2
    hostname: odb2.pendev
    image: orientdb:3.0.34
    command: dserver.sh
    volumes:
      - ./docker/orientdb/config/odb2:/orientdb/config
      - ./docker/orientdb/config/shared:/orientdb/config-shared
      - odb2_data:/orientdb/databases
    environment:
      ORIENTDB_ROOT_PASSWORD: root
      ORIENTDB_NODE_NAME: odb2.pendev
    ports:
      - 2425:2424
      - 2481:2480
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:2480/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      default:
        aliases:
          - odb2.pendev

  odb3:
    container_name: odb3
    hostname: odb3.pendev
    image: orientdb:3.0.34
    command: dserver.sh
    volumes:
      - ./docker/orientdb/config/odb3:/orientdb/config
      - ./docker/orientdb/config/shared:/orientdb/config-shared
      - odb3_data:/orientdb/databases
    environment:
      ORIENTDB_ROOT_PASSWORD: root
      ORIENTDB_NODE_NAME: odb3.pendev
    ports:
      - 2426:2424
      - 2482:2480
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:2480/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      default:
        aliases:
          - odb3.pendev
{{end}}

  db_migration:
    container_name: db_migration
    build: docker/db-migration
    command: /kilda/migrate-develop.sh
    depends_on:
      odb1:
        condition: service_healthy
{{if not (exists "/single_orientdb")}}
      odb2:
        condition: service_healthy
      odb3:
        condition: service_healthy
{{end}}
    healthcheck:
      test: ["CMD-SHELL", "test -f /kilda/flag/migration.ok || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 1
      start_period: 600s

  zookeeper:
    container_name: zookeeper
    hostname: zookeeper.pendev
    image: kilda/zookeeper:latest
    command: /usr/local/sbin/create-zknode-and-start.sh
    volumes:
      - ./docker/zookeeper/bin/create-zknode-and-start.sh:/usr/local/sbin/create-zknode-and-start.sh
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent QuorumPeer"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - zookeeper.pendev
    mem_limit: ${ZOOKEEPER_MEM_LIMIT:-1073741824}
    environment:
      KILDA_ZKNODE: kilda


  kafka:
    container_name: kafka
    hostname: kafka.pendev
    image: kilda/kafka:latest
    # run_and_configure is in services/kafka/kafka-conf
    command: /opt/kafka/bin/run_and_configure.sh
    # command: /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
    volumes:
      - kafka_data:/data/kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    extra_hosts:
      - "kafka.pendev:127.0.0.1"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent Kafka"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - kafka.pendev
    environment:
      KAFKA_HEAP_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XshowSettings:all"
    mem_limit: ${KAFKA_MEM_LIMIT:-1073741824}

  wfm:
    container_name: wfm
    image: "kilda/wfm:${TAG:-latest}"
    build:
      context: docker
      dockerfile: wfm/Dockerfile
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
      storm-supervisor:
        condition: service_healthy
      db_migration:
        condition: service_healthy
    environment:
      - WFM_TOPOLOGIES_MODE

  hbase:
    container_name: hbase
    hostname: hbase.pendev
    image: kilda/hbase:latest
    command: /opt/hbase/bin/start-hbase
    volumes:
      - hbase_data:/data/hbase
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "60000:60000"
      - "60010:60010"
      - "60020:60020"
      - "60030:60030"
      - "8070:8070"
      - "8090:8090"
      - "9070:9070"
      - "9080:9080"
      - "9090:9090"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent HMaster"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - hbase.pendev
    mem_limit: ${HBASE_MEM_LIMIT:-1073741824}

  storm-nimbus:
    container_name: storm-nimbus
    hostname: nimbus.pendev
    image: kilda/storm:latest
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm nimbus
    depends_on:
      zookeeper:
        condition: service_healthy
{{if not (exists "/no_opentsdb")}}      opentsdb:
        condition: service_started
{{end}}    ports:
      - "6627:6627"
      - "3772:3772"
      - "3773:3773"
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent nimbus"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-nimbus.pendev
         - nimbus.pendev

{{if not (exists "/no_storm_ui")}}
  storm-ui:
    container_name: storm-ui
    image: kilda/storm:latest
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm ui
    depends_on:
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
      storm-supervisor:
        condition: service_healthy
    ports:
      - "8888:8080"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent core"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-ui.pendev
{{end}}

  storm-supervisor:
    container_name: storm-supervisor
    hostname: storm-supervisor.pendev
    image: kilda/storm:latest
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm supervisor
    depends_on:
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}{{if not (exists "/no_opentsdb")}}      opentsdb:
        condition: service_started
{{end}}    ports:
      - "6700:6700"
      - "6701:6701"
      - "6702:6702"
      - "6703:6703"
      - "6704:6704"
      - "6705:6705"
      - "6706:6706"
      - "6707:6707"
      - "6708:6708"
      - "6709:6709"
      - "6710:6710"
      - "6711:6711"
      - "6712:6712"
      - "6713:6713"
      - "6714:6714"
      - "8001:8000"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent Supervisor"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-supervisor.pendev


  floodlight_1:
    container_name: floodlight_1
    build:
      context: docker
      dockerfile: floodlight-modules/Dockerfile
    image: "kilda/floodlight:${TAG:-latest}"
    command: ["floodlight", "-Dorg.openkilda.floodlight.KafkaChannel.floodlight-region=1", "-Dorg.openkilda.floodlight.KafkaChannel.kafka-groupid=floodlight_1"]
    ports:
      - "6653:6653"
      - "8081:8080"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    cap_add:
      - NET_ADMIN
    networks:
      default:
        aliases:
         - floodlight_1.pendev
# we use kilda as hostname for FL in atdd topologies
         - kilda
    mem_limit: ${FL_1_MEM_LIMIT:-1073741824}

{{if not (exists "/no_floodlight_2")}}
  floodlight_2:
    container_name: floodlight_2
    build:
      context: docker
      dockerfile: floodlight-modules/Dockerfile
    image: "kilda/floodlight:${TAG:-latest}"
    command: ["floodlight", "-Dorg.openkilda.floodlight.KafkaChannel.floodlight-region=2", "-Dorg.openkilda.floodlight.KafkaChannel.kafka-groupid=floodlight_2"]
    ports:
      - "6753:6653"
      - "8281:8080"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    cap_add:
      - NET_ADMIN
    networks:
      default:
        aliases:
          - floodlight_2.pendev
    mem_limit: ${FL_2_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_floodlight_stats")}}
  floodlight_stats:
    container_name: floodlight_stats
    build:
      context: docker
      dockerfile: floodlight-modules/Dockerfile
    image: "kilda/floodlight:${TAG:-latest}"
    command:
      - "floodlightStats"
      - "-Dorg.openkilda.floodlight.KafkaChannel.floodlight-region=1.stats"
      - "-Dorg.openkilda.floodlight.KafkaChannel.kafka-groupid=floodlight_1.stats"
    ports:
      - "6654:6653"
      - "8082:8080"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    cap_add:
      - NET_ADMIN
    networks:
      default:
        aliases:
          - floodlight_stats.pendev
    mem_limit: ${FL_STATS_MEM_LIMIT:-1073741824}
{{end}}

  northbound:
    container_name: openkilda-northbound
    image: "kilda/northbound:${TAG:-latest}"
    build:
      context: docker
      dockerfile: northbound/Dockerfile
    ports:
      - "8080:8080"
    environment:
      REST_USERNAME: 'kilda'
      REST_PASSWORD: 'kilda'
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}      db_migration:
        condition: service_healthy
    networks:
      default:
        aliases:
         - northbound.pendev
    mem_limit: ${NB_MEM_LIMIT:-1073741824}

  northbound-blue-green:
    container_name: openkilda-northbound-blue-green
    image: "kilda/northbound:${TAG:-latest}"
    build:
      context: docker
      dockerfile: northbound/Dockerfile
    ports:
      - "8787:8080"
    environment:
      REST_USERNAME: 'kilda'
      REST_PASSWORD: 'kilda'
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}      odb1:
        condition: service_healthy
    networks:
      default:
        aliases:
         - northbound.pendev
    mem_limit: ${NB_MEM_LIMIT:-1073741824}

{{if not (exists "/no_grpc_speaker")}}
  grpc-speaker:
    container_name: grpc-speaker
    image: "kilda/grpc-speaker:${TAG:-latest}"
    build:
      context: docker
      dockerfile: grpc-service/Dockerfile
    ports:
    - "8091:8091"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    networks:
      default:
        aliases:
         - grpc-speaker.pendev
    mem_limit: ${GRPC_MEM_LIMIT:-1073741824}

  grpc-speaker-blue-green:
      container_name: grpc-speaker-blue-green
      image: "kilda/grpc-speaker:${TAG:-latest}"
      build:
        context: docker
        dockerfile: grpc-service/Dockerfile
      ports:
      - "8092:8091"
      depends_on:
        kafka:
          condition: service_healthy
  {{if not (exists "/no_logstash")}}      logstash:
          condition: service_started
  {{end}}    networks:
        default:
          aliases:
           - grpc-speaker.pendev
      mem_limit: ${GRPC_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_opentsdb")}}
  opentsdb:
    container_name: opentsdb
    hostname: opentsdb.pendev
    image: kilda/opentsdb:latest
    command: /app/wait-for-it.sh -t 120 -h hbase.pendev -p 9090 -- /app/start-opentsdb
    depends_on:
      zookeeper:
        condition: service_healthy
      hbase:
        condition: service_healthy
    ports:
      - "4242:4242"
    networks:
      default:
        aliases:
         - opentsdb.pendev
    mem_limit: ${OTSDB_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_gui")}}
  gui:
    container_name: gui
    build:
      context: src-gui
      dockerfile: Dockerfile
    ports:
      - "8010:1010"
    depends_on:
      northbound:
        condition: service_started
{{if not (exists "/no_opentsdb")}}      opentsdb:
        condition: service_started
{{end}}    networks:
      default:
        aliases:
         - openkilda-gui.pendev
    mem_limit: ${GUI_MEM_LIMIT:-1073741824}
{{end}}

{{if not (exists "/no_elasticsearch")}}
  elasticsearch:
    container_name: elasticsearch
    image: kilda/elasticsearch:latest
    hostname: elasticsearch.pendev
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      default:
        aliases:
         - elasticsearch.pendev
    environment:
      ES_JAVA_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
    mem_limit: ${ELASTIC_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_kibana")}}
  kibana:
    image: kibana:5.6.12
    container_name: kibana
    hostname: kibana.pendev{{if not (exists "/no_elasticsearch")}}
    depends_on:
      elasticsearch:
        condition: service_started{{end}}
    ports:
      - 5601:5601
    networks:
      default:
        aliases:
         - kibana.pendev
    mem_limit: ${KIBANA_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_logstash")}}
  logstash:
    container_name: logstash
    hostname: logstash.pendev
    image: kilda/logstash:latest{{if not (exists "/no_elasticsearch")}}
    depends_on:
      elasticsearch:
        condition: service_started{{end}}
    ports:
      - 9600:9600
      - {{ getv "/kilda_logging_port_tests" }}:{{ getv "/kilda_logging_port_tests" }}
    networks:
      default:
        aliases:
         - logstash.pendev
    environment:
      LS_JAVA_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XshowSettings:all"
      LS_OPTS: "--debug-config"
      # remove default 500m heap size in /usr/share/logstash/vendor/jruby/bin/jruby
      JAVA_MEM: " "
    mem_limit: ${LOGSTASH_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_control")}}
  server42-control:
    container_name: server42-control
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-control.jar
    ports:
      - "9002:9002"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-control.pendev
    mem_limit: ${S42C_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_stats")}}
  server42-stats:
    container_name: server42-stats
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-stats.jar
    ports:
      - "9003:9003"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-stats.pendev
    mem_limit: ${S42S_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_server_stub")}}
  server42-server-stub:
    container_name: server42-server-stub
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-control-server-stub.jar
    ports:
      - "5555:5555"
      - "5556:5556"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-server-stub.pendev
    mem_limit: ${S42SS_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_storm_stub")}}
  server42-storm-stub:
    container_name: server42-storm-stub
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-control-storm-stub.jar
    ports:
      - "9001:9001"
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-storm-stub.pendev
    mem_limit: ${S42STS_MEM_LIMIT:-1073741824}
{{end}}

volumes:
  zookeeper_data:
  kafka_data:
  app_server_data:
  hbase_data:
  odb1_data:
{{if not (exists "/single_orientdb")}}
  odb2_data:
  odb3_data:
{{end}}

# Generated by confd.
# Do not change this file, all changes will be lost. Change corresponding template.

version: '2.1'
services:

{{if not (exists "/no_grpc_stub")}}
  grpc-stub:
    container_name: grpc-stub
    image: kilda/grpc-stub
    build:
      context: docker/grpc-stub/
      dockerfile: Dockerfile
    ports:
      - "50051:50051"
    command: python server.py
    networks:
      default:
        aliases:
          - grpc-stub.pendev
    mem_limit: ${GRPC_STUB_MEM_LIMIT:-67108864}
{{end}}

  lab-api:
    container_name: lab-api
    image: kilda_lab-service
    build:
      context: docker/lab-service/
      dockerfile: Dockerfile
    ports:
    - "8288:8288"
    privileged: true
    command: api
    volumes:
    - /var/run/docker.sock:/var/run/docker.sock
    - /lib/modules:/lib/modules
    networks:
      default:
        aliases:
        - lab-api.pendev
    mem_limit: ${LAB_API_MEM_LIMIT:-256741824}

  neo4j:
    container_name: neo4j
    hostname: neo4j.pendev
    image: "kilda/neo4j:${full_build_number:-latest}"
    ports:
      - "7474:7474"
      - "7687:7687"
      - "16010:16010"
      - "16011:16011"
    volumes:
      - neo4j_data:/var/lib/neo4j/data
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:7474/"]
      interval: 30s
      timeout: 10s
      retries: 3
    environment:
      NEO4J_AUTH: "none"
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_apoc_import_file_enabled: "true"
      NEO4J_apoc_import_file_use__neo4j__config: "true"
      NEO4J_dbms_security_procedures_whitelist: "apoc.*"
      NEO4J_dbms_security_allow__csv__import__from__file__urls: "true"
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*"
      NEO4J_dbms_memory_heap_max__size: "512M"
      NEO4J_dbms_memory_pagecache_size: "512M"
      NEO4J_dbms_memory_heap_initial__size: "512M"
      NEO4J_dbms_connectors_default__listen__address: "0.0.0.0"
      NEO4J_dbms_directories_data: "/var/lib/neo4j/data"
    networks:
      default:
        aliases:
         - neo4j.pendev
    mem_limit: ${NEO4J_MEM_LIMIT:-1073741824}


  zookeeper:
    container_name: zookeeper
    hostname: zookeeper.pendev
    image: "kilda/zookeeper:${full_build_number:-latest}"
    command: /opt/zookeeper/bin/zkServer.sh start-foreground
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent QuorumPeer"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - zookeeper.pendev
    mem_limit: ${ZOOKEEPER_MEM_LIMIT:-1073741824}


  kafka:
    container_name: kafka
    hostname: kafka.pendev
    image: "kilda/kafka:${full_build_number:-latest}"
    # run_and_configure is in services/kafka/kafka-conf
    command: /opt/kafka/bin/run_and_configure.sh
    # command: /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
    volumes:
      - kafka_data:/data/kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    extra_hosts:
      - "kafka.pendev:127.0.0.1"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent Kafka"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - kafka.pendev
    environment:
      KAFKA_HEAP_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XshowSettings:all"
    mem_limit: ${KAFKA_MEM_LIMIT:-1073741824}

  wfm:
    container_name: wfm
    build:
      context: docker
      dockerfile: wfm/Dockerfile
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
      storm-supervisor:
        condition: service_healthy


  hbase:
    container_name: hbase
    hostname: hbase.pendev
    image: "kilda/hbase:${full_build_number:-latest}"
    command: /opt/hbase/bin/start-hbase
    volumes:
      - hbase_data:/data/hbase
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "60000:60000"
      - "60010:60010"
      - "60020:60020"
      - "60030:60030"
      - "8070:8070"
      - "8090:8090"
      - "9070:9070"
      - "9080:9080"
      - "9090:9090"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent HMaster"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - hbase.pendev
    mem_limit: ${HBASE_MEM_LIMIT:-1073741824}

  storm-nimbus:
    container_name: storm-nimbus
    hostname: nimbus.pendev
    image: "kilda/storm:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm nimbus
    depends_on:
      zookeeper:
        condition: service_healthy
{{if not (exists "/no_opentsdb")}}      opentsdb:
        condition: service_started
{{end}}    ports:
      - "6627:6627"
      - "3772:3772"
      - "3773:3773"
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent nimbus"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-nimbus.pendev
         - nimbus.pendev

{{if not (exists "/no_storm_ui")}}
  storm-ui:
    container_name: storm-ui
    image: "kilda/storm:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm ui
    depends_on:
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
      storm-supervisor:
        condition: service_healthy
    ports:
      - "8888:8080"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent core"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-ui.pendev
{{end}}

  storm-supervisor:
    container_name: storm-supervisor
    hostname: storm-supervisor.pendev
    image: "kilda/storm:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm supervisor
    depends_on:
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}{{if not (exists "/no_opentsdb")}}      opentsdb:
        condition: service_started
{{end}}    ports:
      - "6700:6700"
      - "6701:6701"
      - "6702:6702"
      - "6703:6703"
      - "6704:6704"
      - "6705:6705"
      - "6706:6706"
      - "6707:6707"
      - "6708:6708"
      - "6709:6709"
      - "6710:6710"
      - "6711:6711"
      - "6712:6712"
      - "6713:6713"
      - "6714:6714"
      - "8001:8000"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent Supervisor"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-supervisor.pendev


  floodlight_1:
    container_name: floodlight_1
    build:
      context: docker
      dockerfile: floodlight-modules/Dockerfile
    image: "kilda/floodlight:${full_build_number:-latest}"
    command: ["floodlight", "-Dorg.openkilda.floodlight.KafkaChannel.floodlight-region=1", "-Dorg.openkilda.floodlight.KafkaChannel.kafka-groupid=floodlight_1"]
    ports:
      - "6653:6653"
      - "8081:8080"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    cap_add:
      - NET_ADMIN
    networks:
      default:
        aliases:
         - floodlight_1.pendev
# we use kilda as hostname for FL in atdd topologies
         - kilda
    mem_limit: ${FL_1_MEM_LIMIT:-1073741824}

{{if not (exists "/no_floodlight_2")}}
  floodlight_2:
    container_name: floodlight_2
    build:
      context: docker
      dockerfile: floodlight-modules/Dockerfile
    image: "kilda/floodlight:${full_build_number:-latest}"
    command: ["floodlight", "-Dorg.openkilda.floodlight.KafkaChannel.floodlight-region=2", "-Dorg.openkilda.floodlight.KafkaChannel.kafka-groupid=floodlight_2"]
    ports:
      - "6753:6653"
      - "8281:8080"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    cap_add:
      - NET_ADMIN
    networks:
      default:
        aliases:
          - floodlight_2.pendev
    mem_limit: ${FL_2_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_floodlight_stats")}}
  floodlight_stats:
    container_name: floodlight_stats
    build:
      context: docker
      dockerfile: floodlight-modules/Dockerfile
    image: "kilda/floodlight:${full_build_number:-latest}"
    command: ["floodlightStats", "-Dorg.openkilda.floodlight.KafkaChannel.floodlight-region=1", "-Dorg.openkilda.floodlight.KafkaChannel.kafka-groupid=floodlight_1"]
    ports:
      - "6654:6653"
      - "8082:8080"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    cap_add:
      - NET_ADMIN
    networks:
      default:
        aliases:
          - floodlight_stats.pendev
    mem_limit: ${FL_STATS_MEM_LIMIT:-1073741824}
{{end}}
  northbound:
    container_name: openkilda-northbound
    build:
      context: docker
      dockerfile: northbound/Dockerfile
    ports:
      - "8080:8080"
    environment:
      REST_USERNAME: 'kilda'
      REST_PASSWORD: 'kilda'
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}      neo4j:
        condition: service_healthy
    networks:
      default:
        aliases:
         - northbound.pendev
    mem_limit: ${NB_MEM_LIMIT:-1073741824}

{{if not (exists "/no_grpc_speaker")}}
  grpc-speaker:
    container_name: grpc-speaker
    build:
      context: docker
      dockerfile: grpc-service/Dockerfile
    ports:
    - "8091:8091"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
{{end}}    networks:
      default:
        aliases:
         - grpc-speaker.pendev
    mem_limit: ${GRPC_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_opentsdb")}}
  opentsdb:
    container_name: opentsdb
    hostname: opentsdb.pendev
    image: "kilda/opentsdb:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h hbase.pendev -p 9090 -- /app/start-opentsdb
    depends_on:
      zookeeper:
        condition: service_healthy
      hbase:
        condition: service_healthy
    ports:
      - "4242:4242"
    networks:
      default:
        aliases:
         - opentsdb.pendev
    mem_limit: ${OTSDB_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_gui")}}
  gui:
    container_name: gui
    build:
      context: src-gui
      dockerfile: Dockerfile
    ports:
      - "8010:1010"
    depends_on:
      northbound:
        condition: service_started
{{if not (exists "/no_opentsdb")}}      opentsdb:
        condition: service_started
{{end}}    networks:
      default:
        aliases:
         - openkilda-gui.pendev
    mem_limit: ${GUI_MEM_LIMIT:-1073741824}
{{end}}

{{if not (exists "/no_elasticsearch")}}
  elasticsearch:
    container_name: elasticsearch
    image: "kilda/elasticsearch:${full_build_number:-latest}"
    hostname: elasticsearch.pendev
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      default:
        aliases:
         - elasticsearch.pendev
    environment:
      ES_JAVA_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
    mem_limit: ${ELASTIC_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_kibana")}}
  kibana:
    image: kibana:5.6.12
    container_name: kibana
    hostname: kibana.pendev{{if not (exists "/no_elasticsearch")}}
    depends_on:
      elasticsearch:
        condition: service_started{{end}}
    ports:
      - 5601:5601
    networks:
      default:
        aliases:
         - kibana.pendev
    mem_limit: ${KIBANA_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_logstash")}}
  logstash:
    container_name: logstash
    hostname: logstash.pendev
    image: "kilda/logstash:${full_build_number:-latest}"{{if not (exists "/no_elasticsearch")}}
    depends_on:
      elasticsearch:
        condition: service_started{{end}}
    ports:
      - 9600:9600
    networks:
      default:
        aliases:
         - logstash.pendev
    environment:
      LS_JAVA_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XshowSettings:all"
      LS_OPTS: "--debug-config"
      # remove default 500m heap size in /usr/share/logstash/vendor/jruby/bin/jruby
      JAVA_MEM: " "
    mem_limit: ${LOGSTASH_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_control")}}
  server42-control:
    container_name: server42-control
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-control.jar
    ports:
      - "9002:9002"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-control.pendev
    mem_limit: ${NB_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_stats")}}
  server42-stats:
    container_name: server42-stats
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-stats.jar
    ports:
      - "9003:9003"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-stats.pendev
    mem_limit: ${NB_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_server_stub")}}
  server42-server-stub:
    container_name: server42-server-stub
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-control-server-stub.jar
    ports:
      - "5555:5555"
      - "5556:5556"
    depends_on:
      kafka:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-server-stub.pendev
    mem_limit: ${NB_MEM_LIMIT:-1073741824}
{{end}}
{{if not (exists "/no_server42_storm_stub")}}
  server42-storm-stub:
    container_name: server42-storm-stub
    build:
      context: docker
      dockerfile: server42/Dockerfile
    image: kilda/server42
    command: java -XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -jar server42-control-storm-stub.jar
    ports:
      - "9001:9001"
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
{{if not (exists "/no_logstash")}}      logstash:
        condition: service_started
  {{end}}
    networks:
      default:
        aliases:
         - server42-storm-stub.pendev
    mem_limit: ${NB_MEM_LIMIT:-1073741824}
{{end}}

volumes:
  zookeeper_data:
  kafka_data:
  app_server_data:
  hbase_data:
  neo4j_data:

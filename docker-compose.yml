version: '2.1'
services:

  lab-api:
    container_name: lab-api
    image: kilda_lab-service
    build:
      context: services/lab-service/
    ports:
    - "8288:8288"
    privileged: true
    command: api
    volumes:
    - /var/run/docker.sock:/var/run/docker.sock
    networks:
      default:
        aliases:
        - lab-api.pendev
    mem_limit: 256741824

  neo4j:
    container_name: neo4j
    hostname: neo4j.pendev
    image: "kilda/neo4j:${full_build_number:-latest}"
    ports:
      - "7474:7474"
      - "7687:7687"
      - "16010:16010"
      - "16011:16011"
    volumes:
      - neo4j_data:/var/lib/neo4j/data
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:7474/"]
      interval: 30s
      timeout: 10s
      retries: 3
    environment:
      NEO4J_AUTH: "none"
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_apoc_import_file_enabled: "true"
      NEO4J_apoc_import_file_use__neo4j__config: "true"
      NEO4J_dbms_security_procedures_whitelist: "apoc.*"
      NEO4J_dbms_security_allow__csv__import__from__file__urls: "true"
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*"
      NEO4J_dbms_memory_heap_max__size: "1024M"
    networks:
      default:
        aliases:
         - neo4j.pendev
    mem_limit: 1073741824

##
## Python based topology-engine
##
  topology-engine:
    container_name: topology-engine
    hostname: topology-engine.pendev
    build:
      context: services/topology-engine/
      dockerfile: Dockerfile
    image: "kilda/topology-engine:${full_build_number:-latest}"
    command: ./entrypoint.sh
    environment:
      neo4jhost: 'neo4j'
      neo4juser: 'neo4j'
      neo4jpass: 'temppass'
      neo4jbolt: 'http://neo4j:7474/db/data/cypher'
      bootstrapserver: 'kafka.pendev:9092'
      speaker_in_topic: 'speaker.in'
      group: 'python-tpe-tl-consumer'
      OK_TESTS: "${OK_TESTS:-none}"
    depends_on:
      kafka:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      logstash:
        condition: service_started
    networks:
      default:
        aliases:
         - topology-engine.pendev
    mem_limit: 1073741824


  zookeeper:
    container_name: zookeeper
    hostname: zookeeper.pendev
    image: "kilda/zookeeper:${full_build_number:-latest}"
    command: /opt/zookeeper/bin/zkServer.sh start-foreground
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent QuorumPeer"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - zookeeper.pendev
    mem_limit: 1073741824


  kafka:
    container_name: kafka
    hostname: kafka.pendev
    image: "kilda/kafka:${full_build_number:-latest}"
    # run_and_configure is in services/kafka/kafka-conf
    command: /opt/kafka/bin/run_and_configure.sh
    # command: /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
    volumes:
      - kafka_data:/data/kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    extra_hosts:
      - "kafka.pendev:127.0.0.1"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent Kafka"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - kafka.pendev
    environment:
      KAFKA_HEAP_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XshowSettings:all"
    mem_limit: 1073741824

  wfm:
    container_name: wfm
    build:
      context: services/wfm
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
      storm-supervisor:
        condition: service_healthy


  hbase:
    container_name: hbase
    hostname: hbase.pendev
    image: "kilda/hbase:${full_build_number:-latest}"
    command: /opt/hbase/bin/start-hbase
    volumes:
      - hbase_data:/data/hbase
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "60000:60000"
      - "60010:60010"
      - "60020:60020"
      - "60030:60030"
      - "8070:8070"
      - "8090:8090"
      - "9070:9070"
      - "9080:9080"
      - "9090:9090"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent HMaster"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - hbase.pendev
    mem_limit: 1073741824

  storm-nimbus:
    container_name: storm-nimbus
    hostname: nimbus.pendev
    image: "kilda/storm:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm nimbus
    depends_on:
      zookeeper:
        condition: service_healthy
      opentsdb:
        condition: service_started
    ports:
      - "6627:6627"
      - "3772:3772"
      - "3773:3773"
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent nimbus"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-nimbus.pendev
         - nimbus.pendev


  storm-ui:
    container_name: storm-ui
    image: "kilda/storm:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm ui
    depends_on:
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
      storm-supervisor:
        condition: service_healthy
    ports:
      - "8888:8080"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent core"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-ui.pendev


  storm-supervisor:
    container_name: storm-supervisor
    hostname: storm-supervisor.pendev
    image: "kilda/storm:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h zookeeper.pendev -p 2181 -- /opt/storm/bin/storm supervisor
    depends_on:
      zookeeper:
        condition: service_healthy
      storm-nimbus:
        condition: service_healthy
      logstash:
        condition: service_started
      opentsdb:
        condition: service_started
    ports:
      - "6700:6700"
      - "6701:6701"
      - "6702:6702"
      - "6703:6703"
      - "6704:6704"
      - "6705:6705"
      - "6706:6706"
      - "6707:6707"
      - "6708:6708"
      - "6709:6709"
      - "6710:6710"
      - "6711:6711"
      - "6712:6712"
      - "6713:6713"
      - "6714:6714"
      - "8001:8000"
    healthcheck:
      test: ["CMD-SHELL", "jps | grep --silent Supervisor"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      default:
        aliases:
         - storm-supervisor.pendev


  floodlight:
    container_name: floodlight
    build:
      context: services/src/floodlight-modules
    image: "kilda/floodlight:${full_build_number:-latest}"
    command: ["floodlight", "-Dorg.openkilda.floodlight.KafkaChannel.floodlight-region="]
    ports:
      - "6653:6653"
      - "8180:8080"
      - "6655:6655"
      - "6642:6642"
      - "8081:8080"
    depends_on:
      kafka:
        condition: service_healthy
      logstash:
        condition: service_started
    networks:
      default:
        aliases:
         - floodlight.pendev
# we use kilda as hostname for FL in atdd topologies
         - kilda
    mem_limit: 1073741824

  northbound:
    container_name: openkilda-northbound
    build:
      context: services/src/northbound/
    ports:
      - "8080:8080"
    environment:
      REST_USERNAME: 'kilda'
      REST_PASSWORD: 'kilda'
    depends_on:
      kafka:
        condition: service_healthy
      logstash:
        condition: service_started
      neo4j:
        condition: service_healthy
    networks:
      default:
        aliases:
         - northbound.pendev
    mem_limit: 1073741824


  opentsdb:
    container_name: opentsdb
    hostname: opentsdb.pendev
    image: "kilda/opentsdb:${full_build_number:-latest}"
    command: /app/wait-for-it.sh -t 120 -h hbase.pendev -p 9090 -- /app/start-opentsdb
    depends_on:
      zookeeper:
        condition: service_healthy
      hbase:
        condition: service_healthy
    ports:
      - "4242:4242"
    networks:
      default:
        aliases:
         - opentsdb.pendev
    mem_limit: 1073741824


  gui:
    container_name: gui
    build:
      context: services/src/openkilda-gui
    ports:
      - "8010:1010"
    depends_on:
      northbound:
        condition: service_started
      opentsdb:
        condition: service_started
    networks:
      default:
        aliases:
         - openkilda-gui.pendev
    mem_limit: 1073741824

  elasticsearch:
    container_name: elasticsearch
    image: "kilda/elasticsearch:${full_build_number:-latest}"
    build:
      context: services/elasticsearch/
      dockerfile: Dockerfile
    hostname: elasticsearch.pendev
    ports:
      - "127.0.0.1:9200:9200"
      - "127.0.0.1:9300:9300"
    networks:
      default:
        aliases:
         - elasticsearch.pendev
    environment:
      ES_JAVA_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
    mem_limit: 1073741824


  kibana:
    image: kibana:5.6.12
    container_name: kibana
    hostname: kibana.pendev
    depends_on:
      elasticsearch:
        condition: service_started
    ports:
      - 5601:5601
    networks:
      default:
        aliases:
         - kibana.pendev
    mem_limit: 1073741824


  logstash:
    container_name: logstash
    hostname: logstash.pendev
    image: "kilda/logstash:${full_build_number:-latest}"
    depends_on:
      elasticsearch:
        condition: service_started
    ports:
      - 9600:9600
    networks:
      default:
        aliases:
         - logstash.pendev
    environment:
      LS_JAVA_OPTS: "-XX:+PrintFlagsFinal -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XshowSettings:all"
      LS_OPTS: "--debug-config"
      # remove default 500m heap size in /usr/share/logstash/vendor/jruby/bin/jruby
      JAVA_MEM: " "
    mem_limit: 1073741824

volumes:
  zookeeper_data:
  kafka_data:
  app_server_data:
  hbase_data:
  neo4j_data:
